{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47aa8a76",
   "metadata": {},
   "source": [
    "# Housing Data Correlation Analysis\n",
    "\n",
    "**Comprehensive correlation and association analysis before model development**\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "This notebook performs a thorough correlation analysis of the housing dataset to:\n",
    "- Identify relationships between variables\n",
    "- Detect multicollinearity issues\n",
    "- Analyze categorical associations\n",
    "- Rank feature importance\n",
    "- Provide modeling recommendations\n",
    "\n",
    "## Contents\n",
    "1. [Data Loading and Setup](#1-data-loading-and-setup)\n",
    "2. [Exploratory Data Overview](#2-exploratory-data-overview)\n",
    "3. [Correlation Matrix Analysis](#3-correlation-matrix-analysis)\n",
    "4. [High Correlation Detection](#4-high-correlation-detection)\n",
    "5. [Multicollinearity Analysis](#5-multicollinearity-analysis)\n",
    "6. [Categorical Variable Associations](#6-categorical-variable-associations)\n",
    "7. [Mixed Variable Analysis](#7-mixed-variable-analysis)\n",
    "8. [Feature Importance Analysis](#8-feature-importance-analysis)\n",
    "9. [Comprehensive Report](#9-comprehensive-report)\n",
    "10. [Modeling Recommendations](#10-modeling-recommendations)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7d079",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91628403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for imports\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "# Import custom correlation analyzer\n",
    "from correlation_analyzer import HousingCorrelationAnalyzer\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Setup completed successfully!\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa20975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned housing data\n",
    "data_path = project_root / 'data' / 'processed' / 'V1' / 'train_cleaned.csv'\n",
    "\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d9ad78",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bd954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data information\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumn Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nMissing Values Summary:\")\n",
    "missing_summary = df.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "if len(missing_summary) > 0:\n",
    "    print(missing_summary.head(10))\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f284342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical variables\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Remove ID columns from numeric if present\n",
    "id_cols = [col for col in numeric_cols if 'id' in col.lower()]\n",
    "numeric_cols = [col for col in numeric_cols if col not in id_cols]\n",
    "\n",
    "print(f\"Numeric variables ({len(numeric_cols)}): {numeric_cols[:10]}{'...' if len(numeric_cols) > 10 else ''}\")\n",
    "print(f\"Categorical variables ({len(categorical_cols)}): {categorical_cols[:10]}{'...' if len(categorical_cols) > 10 else ''}\")\n",
    "if id_cols:\n",
    "    print(f\"ID columns excluded: {id_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0866b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick statistical summary for numeric variables\n",
    "print(\"=== NUMERIC VARIABLES SUMMARY ===\")\n",
    "numeric_summary = df[numeric_cols].describe()\n",
    "print(numeric_summary.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df2a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis (assuming SalePrice is the target)\n",
    "target_var = 'SalePrice' if 'SalePrice' in df.columns else None\n",
    "\n",
    "if target_var:\n",
    "    print(f\"=== TARGET VARIABLE ANALYSIS: {target_var} ===\")\n",
    "    print(f\"Statistics:\")\n",
    "    print(df[target_var].describe())\n",
    "    \n",
    "    # Plot target distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(df[target_var], bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_title(f'{target_var} Distribution')\n",
    "    axes[0].set_xlabel(target_var)\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Log-transformed histogram\n",
    "    axes[1].hist(np.log1p(df[target_var]), bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axes[1].set_title(f'Log({target_var}) Distribution')\n",
    "    axes[1].set_xlabel(f'Log({target_var})')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Skewness analysis\n",
    "    from scipy.stats import skew\n",
    "    original_skew = skew(df[target_var])\n",
    "    log_skew = skew(np.log1p(df[target_var]))\n",
    "    print(f\"\\nSkewness Analysis:\")\n",
    "    print(f\"Original {target_var} skewness: {original_skew:.3f}\")\n",
    "    print(f\"Log-transformed skewness: {log_skew:.3f}\")\n",
    "else:\n",
    "    print(\"No target variable identified. Please specify target variable for focused analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e66300",
   "metadata": {},
   "source": [
    "## 3. Correlation Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a60dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the correlation analyzer\n",
    "analyzer = HousingCorrelationAnalyzer(df)\n",
    "\n",
    "print(\"Correlation analyzer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlation matrix\n",
    "print(\"Generating correlation matrix...\")\n",
    "correlation_matrix = analyzer.correlation_matrix_analysis(method='pearson', figsize=(16, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show correlation matrix statistics\n",
    "if not correlation_matrix.empty:\n",
    "    print(\"=== CORRELATION MATRIX STATISTICS ===\")\n",
    "    \n",
    "    # Get upper triangle correlations (excluding diagonal)\n",
    "    upper_tri = correlation_matrix.where(\n",
    "        np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    correlations = upper_tri.stack().dropna()\n",
    "    \n",
    "    print(f\"Total correlation pairs: {len(correlations)}\")\n",
    "    print(f\"Mean absolute correlation: {abs(correlations).mean():.3f}\")\n",
    "    print(f\"Median absolute correlation: {abs(correlations).median():.3f}\")\n",
    "    print(f\"Max correlation: {correlations.max():.3f}\")\n",
    "    print(f\"Min correlation: {correlations.min():.3f}\")\n",
    "    \n",
    "    # Distribution of correlation strengths\n",
    "    abs_corr = abs(correlations)\n",
    "    print(f\"\\nCorrelation Strength Distribution:\")\n",
    "    print(f\"Very weak (< 0.3): {(abs_corr < 0.3).sum()} ({(abs_corr < 0.3).mean()*100:.1f}%)\")\n",
    "    print(f\"Weak (0.3-0.5): {((abs_corr >= 0.3) & (abs_corr < 0.5)).sum()} ({((abs_corr >= 0.3) & (abs_corr < 0.5)).mean()*100:.1f}%)\")\n",
    "    print(f\"Moderate (0.5-0.7): {((abs_corr >= 0.5) & (abs_corr < 0.7)).sum()} ({((abs_corr >= 0.5) & (abs_corr < 0.7)).mean()*100:.1f}%)\")\n",
    "    print(f\"Strong (0.7-0.9): {((abs_corr >= 0.7) & (abs_corr < 0.9)).sum()} ({((abs_corr >= 0.7) & (abs_corr < 0.9)).mean()*100:.1f}%)\")\n",
    "    print(f\"Very strong (≥ 0.9): {(abs_corr >= 0.9).sum()} ({(abs_corr >= 0.9).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541827d",
   "metadata": {},
   "source": [
    "## 4. High Correlation Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f507a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find high correlations\n",
    "print(\"Analyzing high correlations...\")\n",
    "high_correlations = analyzer.find_high_correlations(threshold=0.7, target_var=target_var)\n",
    "\n",
    "if not high_correlations.empty:\n",
    "    print(f\"\\n=== HIGH CORRELATIONS (|r| ≥ 0.7) ===\")\n",
    "    print(f\"Found {len(high_correlations)} highly correlated pairs:\")\n",
    "    print(high_correlations.to_string(index=False))\n",
    "    \n",
    "    # Visualization of high correlations\n",
    "    if len(high_correlations) > 0:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        bars = plt.barh(range(len(high_correlations)), \n",
    "                       high_correlations['Correlation'],\n",
    "                       color=['red' if x < 0 else 'blue' for x in high_correlations['Correlation']])\n",
    "        \n",
    "        # Create labels\n",
    "        labels = [f\"{row['Variable_1']} vs {row['Variable_2']}\" \n",
    "                 for _, row in high_correlations.iterrows()]\n",
    "        \n",
    "        plt.yticks(range(len(high_correlations)), labels)\n",
    "        plt.xlabel('Correlation Coefficient')\n",
    "        plt.title('High Correlations (|r| ≥ 0.7)')\n",
    "        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "        plt.axvline(x=0.7, color='red', linestyle='--', alpha=0.5, label='Threshold')\n",
    "        plt.axvline(x=-0.7, color='red', linestyle='--', alpha=0.5)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No high correlations found with threshold 0.7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try lower threshold if no high correlations found\n",
    "if high_correlations.empty:\n",
    "    print(\"Trying lower threshold (0.5)...\")\n",
    "    moderate_correlations = analyzer.find_high_correlations(threshold=0.5, target_var=target_var)\n",
    "    \n",
    "    if not moderate_correlations.empty:\n",
    "        print(f\"\\n=== MODERATE CORRELATIONS (|r| ≥ 0.5) ===\")\n",
    "        print(f\"Found {len(moderate_correlations)} moderately correlated pairs:\")\n",
    "        print(moderate_correlations.head(10).to_string(index=False))\n",
    "    else:\n",
    "        print(\"No correlations found even with threshold 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027facc3",
   "metadata": {},
   "source": [
    "## 5. Multicollinearity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90879272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multicollinearity using VIF\n",
    "print(\"Analyzing multicollinearity (VIF)...\")\n",
    "vif_results = analyzer.multicollinearity_analysis(threshold=5.0)\n",
    "\n",
    "if not vif_results.empty:\n",
    "    print(f\"\\n=== VARIANCE INFLATION FACTOR (VIF) ANALYSIS ===\")\n",
    "    print(vif_results.to_string(index=False))\n",
    "    \n",
    "    # Count problematic variables\n",
    "    high_vif = vif_results[vif_results['High_Multicollinearity']]\n",
    "    if not high_vif.empty:\n",
    "        print(f\"\\n⚠️ WARNING: {len(high_vif)} variables have VIF > 5.0\")\n",
    "        print(\"These may cause multicollinearity issues:\")\n",
    "        print(high_vif[['Variable', 'VIF', 'VIF_Interpretation']].to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\n✅ No multicollinearity concerns detected (all VIF < 5.0)\")\n",
    "    \n",
    "    # Visualize VIF scores\n",
    "    if len(vif_results) > 1:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        colors = ['red' if x else 'green' for x in vif_results['High_Multicollinearity']]\n",
    "        \n",
    "        bars = plt.barh(range(len(vif_results)), vif_results['VIF'], color=colors, alpha=0.7)\n",
    "        plt.yticks(range(len(vif_results)), vif_results['Variable'])\n",
    "        plt.xlabel('VIF Score')\n",
    "        plt.title('Variance Inflation Factor (VIF) by Variable')\n",
    "        plt.axvline(x=5, color='red', linestyle='--', alpha=0.7, label='Threshold (VIF=5)')\n",
    "        plt.axvline(x=10, color='darkred', linestyle='--', alpha=0.7, label='High concern (VIF=10)')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (bar, vif_val) in enumerate(zip(bars, vif_results['VIF'])):\n",
    "            plt.text(vif_val + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{vif_val:.1f}', va='center', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"VIF analysis could not be performed (insufficient data or statsmodels not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b2c5b",
   "metadata": {},
   "source": [
    "## 6. Categorical Variable Associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c035921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical variable associations\n",
    "if len(categorical_cols) > 1:\n",
    "    print(\"Analyzing categorical variable associations...\")\n",
    "    categorical_associations = analyzer.categorical_associations_analysis()\n",
    "    \n",
    "    if not categorical_associations.empty:\n",
    "        print(f\"\\n=== CATEGORICAL ASSOCIATIONS (Cramér's V) ===\")\n",
    "        print(f\"Analyzed {len(categorical_associations)} categorical variable pairs\")\n",
    "        print(\"\\nTop 15 associations:\")\n",
    "        print(categorical_associations.head(15).to_string())\n",
    "        \n",
    "        # Strong associations\n",
    "        strong_associations = categorical_associations[categorical_associations > 0.5]\n",
    "        if len(strong_associations) > 0:\n",
    "            print(f\"\\nStrong associations (Cramér's V > 0.5): {len(strong_associations)}\")\n",
    "            print(strong_associations.to_string())\n",
    "        else:\n",
    "            print(\"\\nNo strong categorical associations found (all Cramér's V ≤ 0.5)\")\n",
    "        \n",
    "        # Visualize top associations\n",
    "        if len(categorical_associations) > 0:\n",
    "            top_10 = categorical_associations.head(10)\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            bars = plt.barh(range(len(top_10)), top_10.values)\n",
    "            plt.yticks(range(len(top_10)), top_10.index)\n",
    "            plt.xlabel(\"Cramér's V\")\n",
    "            plt.title(\"Top 10 Categorical Variable Associations\")\n",
    "            plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Strong association (0.5)')\n",
    "            plt.legend()\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, (bar, val) in enumerate(zip(bars, top_10.values)):\n",
    "                plt.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                        f'{val:.3f}', va='center', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No categorical associations could be calculated\")\n",
    "else:\n",
    "    print(\"Insufficient categorical variables for association analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3befaf",
   "metadata": {},
   "source": [
    "## 7. Mixed Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationships between categorical and numeric variables\n",
    "if target_var and len(categorical_cols) > 0:\n",
    "    print(\"Analyzing categorical vs numeric relationships...\")\n",
    "    \n",
    "    # Test top categorical variables against target\n",
    "    mixed_results = {}\n",
    "    \n",
    "    for cat_var in categorical_cols[:8]:  # Analyze top 8 categorical variables\n",
    "        try:\n",
    "            result = analyzer.mixed_variable_analysis(cat_var, [target_var])\n",
    "            if not result.empty:\n",
    "                mixed_results[cat_var] = result.iloc[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Could not analyze {cat_var}: {e}\")\n",
    "    \n",
    "    if mixed_results:\n",
    "        mixed_df = pd.DataFrame(mixed_results).T\n",
    "        mixed_df = mixed_df.sort_values('eta_squared', ascending=False)\n",
    "        \n",
    "        print(f\"\\n=== CATEGORICAL vs {target_var} ANALYSIS ===\")\n",
    "        print(\"Effect sizes (η²) and significance:\")\n",
    "        display_cols = ['F_statistic', 'p_value', 'eta_squared', 'effect_size', 'significant_005']\n",
    "        print(mixed_df[display_cols].round(4).to_string())\n",
    "        \n",
    "        # Visualize effect sizes\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        colors = ['red' if x else 'blue' for x in mixed_df['significant_005']]\n",
    "        bars = plt.bar(range(len(mixed_df)), mixed_df['eta_squared'], color=colors, alpha=0.7)\n",
    "        plt.xticks(range(len(mixed_df)), mixed_df.index, rotation=45, ha='right')\n",
    "        plt.ylabel('Eta-squared (η²)')\n",
    "        plt.title(f'Effect Sizes: Categorical Variables vs {target_var}')\n",
    "        \n",
    "        # Add significance indicators\n",
    "        for i, (bar, significant) in enumerate(zip(bars, mixed_df['significant_005'])):\n",
    "            if significant:\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                        '*', ha='center', va='bottom', fontsize=16, color='red')\n",
    "        \n",
    "        plt.axhline(y=0.01, color='gray', linestyle='--', alpha=0.5, label='Small effect')\n",
    "        plt.axhline(y=0.06, color='orange', linestyle='--', alpha=0.5, label='Medium effect')\n",
    "        plt.axhline(y=0.14, color='red', linestyle='--', alpha=0.5, label='Large effect')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show significant relationships\n",
    "        significant_vars = mixed_df[mixed_df['significant_005']]\n",
    "        if not significant_vars.empty:\n",
    "            print(f\"\\nSignificant relationships with {target_var} (p < 0.05): {len(significant_vars)}\")\n",
    "            print(significant_vars[['eta_squared', 'effect_size']].to_string())\n",
    "else:\n",
    "    print(\"Mixed variable analysis skipped (no target variable or categorical variables)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fea6b3",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "if target_var:\n",
    "    print(\"Analyzing feature importance...\")\n",
    "    \n",
    "    # Try both correlation and mutual information methods\n",
    "    methods = ['correlation', 'mutual_info']\n",
    "    importance_results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        try:\n",
    "            importance = analyzer.feature_importance_analysis(target_var, method=method)\n",
    "            if not importance.empty:\n",
    "                importance_results[method] = importance\n",
    "                print(f\"\\n=== FEATURE IMPORTANCE: {method.upper()} METHOD ===\")\n",
    "                print(\"Top 15 features:\")\n",
    "                print(importance.head(15).to_string(index=False))\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate {method} importance: {e}\")\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    if importance_results:\n",
    "        for method, importance in importance_results.items():\n",
    "            if len(importance) > 1:\n",
    "                top_features = importance.head(15)\n",
    "                \n",
    "                plt.figure(figsize=(12, 8))\n",
    "                colors = ['orange' if ft == 'Categorical' else 'blue' \n",
    "                         for ft in top_features.get('Feature_Type', ['Numeric'] * len(top_features))]\n",
    "                \n",
    "                bars = plt.barh(range(len(top_features)), \n",
    "                               top_features.iloc[:, 1], color=colors, alpha=0.7)\n",
    "                plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "                plt.xlabel(f'Importance Score ({method})')\n",
    "                plt.title(f'Top 15 Features by {method.title()} Importance')\n",
    "                plt.gca().invert_yaxis()\n",
    "                \n",
    "                # Add value labels\n",
    "                for i, (bar, val) in enumerate(zip(bars, top_features.iloc[:, 1])):\n",
    "                    plt.text(val + max(top_features.iloc[:, 1]) * 0.01, \n",
    "                            bar.get_y() + bar.get_height()/2, \n",
    "                            f'{val:.3f}', va='center', fontsize=9)\n",
    "                \n",
    "                if 'Feature_Type' in top_features.columns:\n",
    "                    # Add legend for feature types\n",
    "                    from matplotlib.patches import Patch\n",
    "                    legend_elements = [Patch(facecolor='blue', alpha=0.7, label='Numeric'),\n",
    "                                     Patch(facecolor='orange', alpha=0.7, label='Categorical')]\n",
    "                    plt.legend(handles=legend_elements)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "else:\n",
    "    print(\"Feature importance analysis skipped (no target variable specified)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db64ebf5",
   "metadata": {},
   "source": [
    "## 9. Comprehensive Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc18fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "print(\"Generating comprehensive correlation analysis report...\")\n",
    "\n",
    "# Create plots directory\n",
    "plots_dir = project_root / 'notebooks' / 'correlation_analysis_plots'\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate full report\n",
    "report_results = analyzer.generate_comprehensive_report(\n",
    "    target_var=target_var,\n",
    "    correlation_threshold=0.7,\n",
    "    vif_threshold=5.0,\n",
    "    save_plots=True,\n",
    "    plots_dir=str(plots_dir)\n",
    ")\n",
    "\n",
    "print(f\"\\nPlots saved to: {plots_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7396cd7",
   "metadata": {},
   "source": [
    "## 10. Modeling Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c58f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display detailed modeling recommendations\n",
    "print(\"=\" * 60)\n",
    "print(\"DETAILED MODELING RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# 1. Feature Selection Recommendations\n",
    "print(\"\\n1. FEATURE SELECTION RECOMMENDATIONS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'feature_importance' in report_results and not report_results['feature_importance'].empty:\n",
    "    importance_df = report_results['feature_importance']\n",
    "    top_features = importance_df.head(20)['Feature'].tolist()\n",
    "    print(f\"• Focus on top 20 features with highest importance:\")\n",
    "    print(f\"  {', '.join(top_features[:10])}\")\n",
    "    if len(top_features) > 10:\n",
    "        print(f\"  {', '.join(top_features[10:])}\")\n",
    "    recommendations.append(\"Use feature importance ranking for initial feature selection\")\n",
    "\n",
    "# 2. Multicollinearity Handling\n",
    "print(\"\\n2. MULTICOLLINEARITY HANDLING\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'vif_analysis' in report_results and not report_results['vif_analysis'].empty:\n",
    "    high_vif_vars = report_results['vif_analysis'][report_results['vif_analysis']['High_Multicollinearity']]\n",
    "    if not high_vif_vars.empty:\n",
    "        print(f\"• Remove or combine these {len(high_vif_vars)} high-VIF variables:\")\n",
    "        for _, row in high_vif_vars.iterrows():\n",
    "            print(f\"  - {row['Variable']} (VIF: {row['VIF']:.2f})\")\n",
    "        recommendations.append(\"Address multicollinearity before modeling\")\n",
    "    else:\n",
    "        print(\"• ✅ No multicollinearity issues detected\")\n",
    "        recommendations.append(\"Multicollinearity is not a concern for this dataset\")\n",
    "\n",
    "# 3. Correlation-based Recommendations\n",
    "print(\"\\n3. CORRELATION-BASED RECOMMENDATIONS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'high_correlations' in report_results and not report_results['high_correlations'].empty:\n",
    "    high_corr_df = report_results['high_correlations']\n",
    "    print(f\"• Consider feature pairs with high correlation ({len(high_corr_df)} pairs):\")\n",
    "    for _, row in high_corr_df.head(5).iterrows():\n",
    "        print(f\"  - {row['Variable_1']} vs {row['Variable_2']} (r={row['Correlation']:.3f})\")\n",
    "    recommendations.append(\"Use regularization or feature selection to handle correlated features\")\n",
    "else:\n",
    "    print(\"• ✅ No problematic high correlations found\")\n",
    "    recommendations.append(\"Feature correlations are manageable\")\n",
    "\n",
    "# 4. Target Variable Recommendations\n",
    "if target_var:\n",
    "    print(f\"\\n4. TARGET VARIABLE ({target_var}) RECOMMENDATIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check skewness\n",
    "    if target_var in df.columns:\n",
    "        from scipy.stats import skew\n",
    "        target_skew = skew(df[target_var])\n",
    "        if abs(target_skew) > 1:\n",
    "            print(f\"• Target variable is skewed (skewness: {target_skew:.3f})\")\n",
    "            print(f\"• Consider log transformation: log({target_var})\")\n",
    "            recommendations.append(\"Apply log transformation to target variable\")\n",
    "        else:\n",
    "            print(f\"• Target variable has acceptable skewness ({target_skew:.3f})\")\n",
    "\n",
    "# 5. Model Type Recommendations\n",
    "print(\"\\n5. RECOMMENDED MODEL TYPES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "model_recommendations = []\n",
    "\n",
    "# Based on correlation structure\n",
    "if 'correlation_matrix' in report_results and not report_results['correlation_matrix'].empty:\n",
    "    corr_matrix = report_results['correlation_matrix']\n",
    "    mean_abs_corr = np.abs(corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]).mean()\n",
    "    \n",
    "    if mean_abs_corr > 0.3:\n",
    "        model_recommendations.extend([\n",
    "            \"Ridge Regression (handles multicollinearity)\",\n",
    "            \"Elastic Net (feature selection + multicollinearity)\"\n",
    "        ])\n",
    "    else:\n",
    "        model_recommendations.append(\"Linear Regression (low correlation between features)\")\n",
    "\n",
    "# Based on feature count\n",
    "num_features = len(numeric_cols) + len(categorical_cols)\n",
    "if num_features > 50:\n",
    "    model_recommendations.extend([\n",
    "        \"Lasso Regression (automatic feature selection)\",\n",
    "        \"Random Forest (handles many features well)\"\n",
    "    ])\n",
    "elif num_features > 20:\n",
    "    model_recommendations.extend([\n",
    "        \"Random Forest\",\n",
    "        \"Gradient Boosting\"\n",
    "    ])\n",
    "\n",
    "# Based on categorical variables\n",
    "if len(categorical_cols) > 5:\n",
    "    model_recommendations.extend([\n",
    "        \"Random Forest (handles categorical features)\",\n",
    "        \"XGBoost (with proper encoding)\"\n",
    "    ])\n",
    "\n",
    "print(f\"• Recommended models based on data characteristics:\")\n",
    "for model in set(model_recommendations):\n",
    "    print(f\"  - {model}\")\n",
    "\n",
    "# 6. Data Preprocessing Recommendations\n",
    "print(\"\\n6. DATA PREPROCESSING RECOMMENDATIONS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "preprocessing_steps = [\n",
    "    \"Standard scaling for numeric features (especially for linear models)\",\n",
    "    \"One-hot encoding for categorical features with few categories\",\n",
    "    \"Target encoding for high-cardinality categorical features\"\n",
    "]\n",
    "\n",
    "if target_var and abs(skew(df[target_var])) > 1:\n",
    "    preprocessing_steps.insert(0, f\"Log transformation for {target_var}\")\n",
    "\n",
    "if 'high_correlations' in report_results and not report_results['high_correlations'].empty:\n",
    "    preprocessing_steps.append(\"PCA or feature selection for correlated features\")\n",
    "\n",
    "for step in preprocessing_steps:\n",
    "    print(f\"• {step}\")\n",
    "\n",
    "# 7. Cross-validation Strategy\n",
    "print(\"\\n7. CROSS-VALIDATION STRATEGY\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"• Use 5-fold cross-validation for model selection\")\n",
    "print(f\"• Stratified sampling if target has skewed distribution\")\n",
    "print(f\"• Time-based split if temporal features are present\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETE - READY FOR MODELING\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae1786",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive correlation analysis has provided insights into:\n",
    "\n",
    "1. **Data Structure**: Understanding of numeric vs categorical variables\n",
    "2. **Correlation Patterns**: Identification of variable relationships\n",
    "3. **Multicollinearity**: Detection of problematic feature combinations\n",
    "4. **Feature Importance**: Ranking of variables by predictive potential\n",
    "5. **Modeling Strategy**: Data-driven recommendations for model selection\n",
    "\n",
    "**Next Steps:**\n",
    "- Implement recommended preprocessing steps\n",
    "- Start with suggested model types\n",
    "- Use identified important features for initial models\n",
    "- Address any multicollinearity concerns\n",
    "- Consider target variable transformations if needed\n",
    "\n",
    "The analysis results and plots have been saved for reference during model development."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
